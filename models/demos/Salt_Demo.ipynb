{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salt Demo\n",
    "\n",
    "In this demo notebook, we will go over how to use the salt model in Open Seismic with the F3 data block (real seismic data). Note that we will be drawing on knowledge that is introduced in the Open Seismic examples. If you have not reviewed those examples, please review them first and come back. \n",
    "\n",
    "## Sections\n",
    "Salt.1 **Using the Default Salt Model**<br/>\n",
    "Salt.2 **Using the Salt Model with Custom Model Script**<br/>\n",
    "Salt.3 **Using the Salt Model with Custom Processor Scripts**\n",
    "\n",
    "## Imports\n",
    "\n",
    "Here are some initial imports and global variables that we need to set up before moving forward. Make sure to run `pip install -r requirements.txt` and build/pull the Docker image for Open Seismic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import PurePath\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        if 'ipynb' in root or 'ipynb' in dirs or 'ipynb' in files:\n",
    "            continue\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "            \n",
    "open_seismic_docker = 'open_seismic'\n",
    "assets_path = PurePath(os.getcwd()).parent.joinpath('salt')\n",
    "os_path = PurePath(os.getcwd()).parent.parent\n",
    "\n",
    "models_path = os_path.joinpath('models')\n",
    "#models_mount = f' -v {models_path}:/core/python/models/'\n",
    "models_mount = f' -v {models_path}:/core/models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Salt.1: Using the Default Salt Model\n",
    "\n",
    "In this section, we will go over how to use the default salt model that exists within Open Seismic. \n",
    "\n",
    "### Steps\n",
    "1. Construct the JSON config file for directing Open Seismic to the necessary files. For this demo, we have already constructed the JSON configuration. Run the cell below to display the JSON config file. We will be using coarse cube inference as our inference task of choice because this aligns with our intention of inference over a large piece of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"inference_params\": {\n",
      "        \"data\": \"data_mnt/Dutch_F3_data/\",\n",
      "        \"given_model\": \"salt\",\n",
      "        \"infer_type\": \"fine_cube_sync\",\n",
      "        \"return_to_fullsize\": \"True\",\n",
      "        \"output\": \"salt_demo_output\",\n",
      "        \"benchmarking\": \"\"\n",
      "    },\n",
      "    \"visualize_params\": {\n",
      "        \"input\": \"salt_demo_output\",\n",
      "        \"output\": \"visualization\",\n",
      "        \"model_type\": \"salt\",\n",
      "        \"slice_no\": \"42\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_path = assets_path.joinpath('salt_config.json')\n",
    "with open(str(json_path), 'rb') as f:\n",
    "    json_config = json.load(f)\n",
    "print(json.dumps(json_config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell, you might notice that we direct Open Seismic to look at the `data_mnt` directory instead of a common directory that we have recommended. The reason for this is because in this demo, we will be using the F3 data block, which a particularly large file and should not be copied multiple times. Additionally, Docker does not support symlinks on certain operating systems. Therefore, we will use one more mount command to mount the F3 data block into the right directory in Open Seismic's Docker container.\n",
    "\n",
    "2. Construct the command to run Open Seismic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'docker run -v /home/sdp/ravi/open_seismic/models/salt:/core/mnt/ -v /home/sdp/ravi/open_seismic/data:/core/data_mnt/ -v /home/sdp/ravi/open_seismic/models/salt/runs:/core/runs/  -v /home/sdp/ravi/open_seismic/models:/core/models/ open_seismic ./run.sh -c /core/mnt/salt_config.json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_dir = assets_path\n",
    "run_dir = assets_path.joinpath('runs')\n",
    "data_dir = os_path.joinpath('data', '')\n",
    "os_input = '/core/mnt/salt_config.json'\n",
    "\n",
    "infer_mount = f'-v {local_dir}:/core/mnt/'\n",
    "runs_mount = f'-v {run_dir}:/core/runs/'\n",
    "data_mount = f'-v {data_dir}:/core/data_mnt/'\n",
    "mounts = f'{infer_mount} {data_mount} {runs_mount} {models_mount}'\n",
    "os_cmd = f\"docker run {mounts} {open_seismic_docker} ./run.sh -c {os_input}\"\n",
    "os_cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run Open Seismic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARG PARSE] model= data=data_mnt/Dutch_F3_data/ output=runs/February25_07-32-00_AM_2021/salt_demo_output\n",
      "[OUTPUT] Making new folder for output storage.\n",
      "[INFER] Setting up inference...\n",
      "[SWAP] Preprocessor not found. Defaulting to given model preprocess.\n",
      "[SWAP] Postprocessor not found. Defaulting to given model postprocess.\n",
      "[SWAP] Custom bin, xml, and model init not found. Using default model init with given model.\n",
      "[INFO] Start benchmarking\n",
      "[INFER] Benchmark output - [Step 1/11] Parsing and validating input arguments\n",
      "[INFER] Benchmark output - [ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README. \n",
      "[INFER] Benchmark output - [Step 2/11] Loading Inference Engine\n",
      "[INFER] Benchmark output - [ INFO ] InferenceEngine:\n",
      "[INFER] Benchmark output -          API version............. 2.1.2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "[INFER] Benchmark output - [ INFO ] Device info\n",
      "[INFER] Benchmark output -          CPU\n",
      "[INFER] Benchmark output -          MKLDNNPlugin............ version 2.1\n",
      "[INFER] Benchmark output -          Build................... 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "[INFER] Benchmark output - \n",
      "[INFER] Benchmark output - [Step 3/11] Setting device configuration\n",
      "[INFER] Benchmark output - [Step 4/11] Reading network files\n",
      "[INFER] Benchmark output - [ INFO ] Read network took 5.36 ms\n",
      "[INFER] Benchmark output - [Step 5/11] Resizing network to match image sizes and given batch\n",
      "[INFER] Benchmark output - [ INFO ] Network batch size: 1\n",
      "[INFER] Benchmark output - [Step 6/11] Configuring input of the model\n",
      "[INFER] Benchmark output - [Step 7/11] Loading the model to the device\n",
      "[INFER] Benchmark output - [ INFO ] Load network took 35.51 ms\n",
      "[INFER] Benchmark output - [Step 8/11] Setting optimal runtime parameters\n",
      "[INFER] Benchmark output - [Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[INFER] Benchmark output - [ INFO ] Network input '0' precision FP32, dimensions (NCDHW): 1 1 65 65 65\n",
      "[INFER] Benchmark output - [ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[INFER] Benchmark output - [ INFO ] Infer Request 0 filling\n",
      "[INFER] Benchmark output - [ INFO ] Fill input '0' with random values (some binary data is expected)\n",
      "[INFER] Benchmark output - [Step 10/11] Measuring performance (Start inference syncronously, limits: 60000 ms duration)\n",
      "[INFER] Benchmark output - [ INFO ] First inference took 0.94 ms\n",
      "[INFER] Benchmark output - [Step 11/11] Dumping statistics report\n",
      "[INFER] Benchmark output - Count:      274945 iterations\n",
      "[INFER] Benchmark output - Duration:   60000.26 ms\n",
      "[INFER] Benchmark output - Latency:    0.21 ms\n",
      "[INFER] Benchmark output - Throughput: 4844.84 FPS\n",
      "[INFER] Benchmark output - /opt/intel/openvino/python/python3.6/openvino/tools/benchmark/main.py:29: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "[INFER] Benchmark output -   logger.warn(\" -nstreams default value is determined automatically for a device. \"\n",
      "[INFER] Benchmark output - /opt/intel/openvino/python/python3.6/openvino/tools/benchmark/utils/inputs_filling.py:71: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "[INFER] Benchmark output -   logger.warn(\"No input files were given: all inputs will be filled with random values!\")\n",
      "[INFO] Error, benchmarking unsuccessful\n",
      "[INFER] Using model: salt_model\n",
      "[LOADER] Loading file: data_mnt/Dutch_F3_data//f3_8bit.segy\n",
      "[INFER] Conducting inference...\n",
      "[INFER] Conducting inference on input: f3_8bit.segy...\n",
      "100%|##########| 71340/71340 [02:05<00:00, 567.58it/s] [INFER] Resizing output to input size...\n",
      "\n",
      "100%|##########| 2/2 [04:35<00:00, 137.63s/it][INFER] Saving output to output path: runs/February25_07-32-00_AM_2021/salt_demo_output/f3_8bit.segy-input/out.npy\n",
      "[INFER] Complete!\n",
      "\n",
      "[VISUALIZATION] Data file runs/February25_07-32-00_AM_2021/salt_demo_output/f3_8bit.segy-input/out.npy -  loaded\n",
      "[VISUALIZATION] Visualization complete! File saved to: runs/February25_07-32-00_AM_2021/visualization/interpretation.png\n"
     ]
    }
   ],
   "source": [
    "! {os_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Salt.2: Using the Salt with Custom Model Script\n",
    "\n",
    "In this section, we will swap out the default model script with our own custom one. You can find the model script in `demos/assets/salt_demo/only_model_script`. \n",
    "\n",
    "### Steps\n",
    "1. Construct the JSON config file for directing Open Seismic to the necessary files. For this demo, we have already constructed the JSON configuration. Run the cell below to display the JSON config file. We will be using coarse cube inference as our inference task of choice because this aligns with our intention of inference over a large piece of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"inference_params\": {\n",
      "        \"data\": \"data_mnt/Dutch_F3_data/\",\n",
      "        \"model\": \"models/salt/only_model_script/\",\n",
      "        \"given_model\": \"salt\",\n",
      "        \"infer_type\": \"fine_cube_sync\",\n",
      "        \"output\": \"salt_model_demo_output\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_path = assets_path.joinpath('salt_model_config.json')\n",
    "with open(str(json_path), 'rb') as f:\n",
    "    json_config = json.load(f)\n",
    "print(json.dumps(json_config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have specified the model script in the `model` parameter of the JSON config file, but we have kept the `salt` option in the `given_model` parameter. In terms of priority, Open Seismic will look at the path specified in `model` first, then will look at the `given_model` files to fill in the required files.\n",
    "\n",
    "2. Construct the command to run Open Seismic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'docker run -v /home/sdp/ravi/open_seismic/models/salt:/core/mnt/ -v /home/sdp/ravi/open_seismic/data:/core/data_mnt/ -v /home/sdp/ravi/open_seismic/models/salt/runs:/core/runs/  -v /home/sdp/ravi/open_seismic/models:/core/models/ open_seismic ./run.sh -c /core/mnt/salt_model_config.json'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_dir = assets_path\n",
    "run_dir = assets_path.joinpath('runs')\n",
    "data_dir = os_path.joinpath('data', '')\n",
    "os_input = '/core/mnt/salt_model_config.json'\n",
    "\n",
    "infer_mount = f'-v {local_dir}:/core/mnt/'\n",
    "runs_mount = f'-v {run_dir}:/core/runs/'\n",
    "data_mount = f'-v {data_dir}:/core/data_mnt/'\n",
    "mounts = f'{infer_mount} {data_mount} {runs_mount} {models_mount}'\n",
    "os_cmd = f\"docker run {mounts} {open_seismic_docker} ./run.sh -c {os_input}\"\n",
    "os_cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run Open Seismic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARG PARSE] model=models/salt/only_model_script/ data=data_mnt/Dutch_F3_data/ output=runs/February25_07-40-06_AM_2021/salt_model_demo_output\n",
      "[OUTPUT] Making new folder for output storage.\n",
      "[INFER] Setting up inference...\n",
      "[SWAP] Preprocessor not found. Defaulting to given model preprocess.\n",
      "[SWAP] Postprocessor not found. Defaulting to given model postprocess.\n",
      "[SWAP] Found custom model script.\n",
      "[SWAP] Custom bin or xml not found. Using given model xml and bin with custom model init.\n",
      "[INFER] Using model: salt_model\n",
      "[LOADER] Loading file: data_mnt/Dutch_F3_data//f3_8bit.segy\n",
      "[INFER] Conducting inference...\n",
      "[INFER] Conducting inference on input: f3_8bit.segy...\n",
      "100%|##########| 71340/71340 [02:06<00:00, 562.96it/s] [INFER] Saving output to output path: runs/February25_07-40-06_AM_2021/salt_model_demo_output/f3_8bit.segy-input/out.npy\n",
      "[INFER] Complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! {os_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Salt.3: Using the Salt with Custom Processor Scripts\n",
    "\n",
    "In this section, we will swap out the default processor scripts with our own custom ones. You can find the model script in `demos/assets/salt_demo/only_processor_scripts`. \n",
    "\n",
    "### Steps\n",
    "1. Construct the JSON config file for directing Open Seismic to the necessary files. For this demo, we have already constructed the JSON configuration. Run the cell below to display the JSON config file. We will be using coarse cube inference as our inference task of choice because this aligns with our intention of inference over a large piece of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"inference_params\": {\n",
      "        \"data\": \"data_mnt/Dutch_F3_data/\",\n",
      "        \"model\": \"mnt/only_processor_scripts/\",\n",
      "        \"given_model\": \"salt\",\n",
      "        \"infer_type\": \"fine_cube_sync\",\n",
      "        \"output\": \"salt_proc_demo_output\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_path = assets_path.joinpath('salt_proc_config.json')\n",
    "with open(str(json_path), 'rb') as f:\n",
    "    json_config = json.load(f)\n",
    "print(json.dumps(json_config, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have specified the model script in the `model` parameter of the JSON config file, but we have kept the `salt` option in the `given_model` parameter. In terms of priority, Open Seismic will look at the path specified in `model` first, then will look at the `given_model` files to fill in the required files.\n",
    "\n",
    "2. Construct the command to run Open Seismic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'docker run -v /home/sdp/ravi/open_seismic/models/salt:/core/mnt/ -v /home/sdp/ravi/open_seismic/data:/core/data_mnt/ -v /home/sdp/ravi/open_seismic/models/salt/runs:/core/runs/  -v /home/sdp/ravi/open_seismic/models:/core/models/ open_seismic ./run.sh -c /core/mnt/salt_proc_config.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_dir = assets_path\n",
    "run_dir = assets_path.joinpath('runs')\n",
    "data_dir = os_path.joinpath('data', '')\n",
    "os_input = '/core/mnt/salt_proc_config.json'\n",
    "\n",
    "infer_mount = f'-v {local_dir}:/core/mnt/'\n",
    "runs_mount = f'-v {run_dir}:/core/runs/'\n",
    "data_mount = f'-v {data_dir}:/core/data_mnt/'\n",
    "mounts = f'{infer_mount} {data_mount} {runs_mount} {models_mount}'\n",
    "os_cmd = f\"docker run {mounts} {open_seismic_docker} ./run.sh -c {os_input}\"\n",
    "os_cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run Open Seismic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ARG PARSE] model=mnt/only_processor_scripts/ data=data_mnt/Dutch_F3_data/ output=runs/February25_07-42-21_AM_2021/salt_proc_demo_output\n",
      "[OUTPUT] Making new folder for output storage.\n",
      "[INFER] Setting up inference...\n",
      "[SWAP] Found custom preporcessing script.\n",
      "[SWAP] Found custom postporcessing script.\n",
      "[SWAP] Custom bin, xml, and model init not found. Using default model init with given model.\n",
      "[INFER] Using model: salt_model\n",
      "[LOADER] Loading file: data_mnt/Dutch_F3_data//f3_8bit.segy\n",
      "[INFER] Conducting inference...\n",
      "[INFER] Conducting inference on input: f3_8bit.segy...\n",
      "100%|##########| 71340/71340 [02:04<00:00, 572.25it/s] [INFER] Saving output to output path: runs/February25_07-42-21_AM_2021/salt_proc_demo_output/f3_8bit.segy-input/out.npy\n",
      "[INFER] Complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! {os_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now know how to use the salt model in Open Seismic.\n",
    "\n",
    "## Summary\n",
    "In this demo, you learned about:\n",
    "1. Using the default salt model with your data\n",
    "2. Using the salt model with your data and model script\n",
    "3. Using the salt model with your data and processor scripts\n",
    "\n",
    "We haven't talked about combining custom model and processor scripts together, but this can be done as well by storing the model and processor scripts in one mount directory. If you would like to learn more about the other models, please go to their respective folders and run their demos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
